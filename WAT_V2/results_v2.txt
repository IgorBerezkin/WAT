============================================================
SHAKESPEARE NEXT TOKEN - LONG SEQUENCES (512+) - Seq2Seq
============================================================
Device: cuda
Dataset: 1,115,394 chars, Vocab: 65
Train: 0 - 50,000 (50,000 samples)
Test:  50,512 - 55,512 (5,000 samples)

WAT embed=40: 105,065 params
Transformer embed=36: 110,513 params
Epochs: 30, LR: 0.0003, Weight Decay: 0.01

============================================================
--- Training WAT ---
============================================================

  Training WAT...
/mnt/d/CODE/else/opencode/WAT_testing/WAT-architecture/benchmarks/shakespeare_long_512.py:137: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler()
/mnt/d/CODE/else/opencode/WAT_testing/WAT-architecture/benchmarks/shakespeare_long_512.py:151: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
/mnt/d/CODE/else/opencode/WAT_testing/WAT-architecture/venv/lib/python3.12/site-packages/torch/nn/functional.py:2954: UserWarning: Mismatch dtype between input and weight: input dtype = c10::Half, weight dtype = float, Cannot dispatch to fused implementation. (Triggered internally at /pytorch/aten/src/ATen/native/layer_norm.cpp:344.)
  return torch.rms_norm(input, normalized_shape, weight, eps)
    Epoch 1/30: loss=2.7774, acc=0.3561, time=37.6s
    Epoch 2/30: loss=2.0680, acc=0.4217, time=35.8s
    Epoch 3/30: loss=1.8193, acc=0.4532, time=35.5s
    Epoch 4/30: loss=1.6666, acc=0.4654, time=35.6s
    Epoch 5/30: loss=1.5645, acc=0.4717, time=35.7s
    Epoch 6/30: loss=1.4922, acc=0.4729, time=35.3s
    Epoch 7/30: loss=1.4373, acc=0.4728, time=36.1s
    Epoch 8/30: loss=1.3948, acc=0.4681, time=35.6s
    Epoch 9/30: loss=1.3612, acc=0.4673, time=35.6s
    Epoch 10/30: loss=1.3336, acc=0.4667, time=35.7s
    Epoch 11/30: loss=1.3112, acc=0.4645, time=35.7s
    Epoch 12/30: loss=1.2931, acc=0.4636, time=35.6s
    Epoch 13/30: loss=1.2784, acc=0.4624, time=35.7s
    Epoch 14/30: loss=1.2652, acc=0.4615, time=35.4s
    Epoch 15/30: loss=1.2533, acc=0.4600, time=35.8s

    === Generated text after epoch 15 ===
    First,
Our Care, and they was a mor what aghafsed should not fools shall parts. Marcius, they to memust I cits,
The vungtly to the Corioly:
Now, and an end all and with that, not what not in will wuvcleice see mine me and with a varm townt consud freed it he wills of that in thee this to be but-likes
Fea
    === End of generation ===

    Epoch 16/30: loss=1.2432, acc=0.4571, time=37.9s
    Epoch 17/30: loss=1.2346, acc=0.4567, time=35.8s
    Epoch 18/30: loss=1.2272, acc=0.4566, time=35.9s
    Epoch 19/30: loss=1.2209, acc=0.4556, time=35.6s
    Epoch 20/30: loss=1.2153, acc=0.4557, time=35.7s

    === Generated text after epoch 20 ===
    First the peats was showeastom must in the godss for a pingloublues Marcius for bated poor of a conting I say. no exclike and yisural talky who hangle in maness to the chanms hear a serve battled to noble leave to Avomanes; do'sfry, I from max-sing hatss
If it
She camour pacnipg acfestlang.

First Soldie
    === End of generation ===

    Epoch 21/30: loss=1.2106, acc=0.4554, time=35.8s
    Epoch 22/30: loss=1.2065, acc=0.4536, time=35.7s
    Epoch 23/30: loss=1.2032, acc=0.4526, time=35.5s
    Epoch 24/30: loss=1.2004, acc=0.4532, time=35.6s
    Epoch 25/30: loss=1.1982, acc=0.4534, time=35.7s
    Epoch 26/30: loss=1.1964, acc=0.4534, time=35.6s
    Epoch 27/30: loss=1.1951, acc=0.4524, time=35.7s
    Epoch 28/30: loss=1.1940, acc=0.4527, time=35.8s
    Epoch 29/30: loss=1.1932, acc=0.4532, time=35.6s
    Epoch 30/30: loss=1.1926, acc=0.4530, time=35.7s

    === Generated text after epoch 30 ===
    First shall the mure; heard. What I would'st thou to'kness sinke unchyed and lead me but undably.

COMINIUS:
As I shall go do report comelr witein out
Farsent one, as words,
What purn and to our their do Rome, do care be had we in the city out when of Marcius?

All:
So lonf on.

MENENIUS:
A have to set c
    === End of generation ===


============================================================
--- Training Transformer
============================================================

  Training Transformer...
    Epoch 1/30: loss=2.8730, acc=0.2787, time=18.3s
    Epoch 2/30: loss=2.4881, acc=0.2854, time=16.0s
    Epoch 3/30: loss=2.4361, acc=0.2878, time=18.2s
    Epoch 4/30: loss=2.4093, acc=0.2907, time=16.0s
    Epoch 5/30: loss=2.3888, acc=0.2938, time=18.3s
    Epoch 6/30: loss=2.3733, acc=0.2968, time=16.0s
    Epoch 7/30: loss=2.3581, acc=0.3018, time=18.2s
    Epoch 8/30: loss=2.3401, acc=0.3079, time=16.3s
    Epoch 9/30: loss=2.3193, acc=0.3154, time=18.0s
    Epoch 10/30: loss=2.2993, acc=0.3207, time=15.6s
    Epoch 11/30: loss=2.2817, acc=0.3274, time=18.8s
    Epoch 12/30: loss=2.2661, acc=0.3315, time=16.0s
    Epoch 13/30: loss=2.2520, acc=0.3367, time=18.2s
    Epoch 14/30: loss=2.2390, acc=0.3413, time=16.0s
    Epoch 15/30: loss=2.2277, acc=0.3439, time=16.0s
    Epoch 16/30: loss=2.2177, acc=0.3477, time=18.3s
    Epoch 17/30: loss=2.2089, acc=0.3491, time=16.0s
    Epoch 18/30: loss=2.2012, acc=0.3510, time=18.2s
    Epoch 19/30: loss=2.1940, acc=0.3540, time=16.0s
    Epoch 20/30: loss=2.1878, acc=0.3544, time=18.3s
    Epoch 21/30: loss=2.1822, acc=0.3568, time=16.0s
    Epoch 22/30: loss=2.1773, acc=0.3573, time=18.2s
    Epoch 23/30: loss=2.1729, acc=0.3589, time=16.0s
    Epoch 24/30: loss=2.1691, acc=0.3598, time=18.3s
    Epoch 25/30: loss=2.1660, acc=0.3604, time=16.0s
    Epoch 26/30: loss=2.1632, acc=0.3616, time=18.3s
    Epoch 27/30: loss=2.1610, acc=0.3617, time=15.9s
    Epoch 28/30: loss=2.1595, acc=0.3621, time=18.3s
    Epoch 29/30: loss=2.1584, acc=0.3625, time=15.9s
    Epoch 30/30: loss=2.1572, acc=0.3628, time=18.3s

============================================================
RESULTS - Shakespeare Next Token (512)
============================================================
WAT:         47.29% (105,065 params)
Transformer: 36.28% (110,513 params)
============================================================
